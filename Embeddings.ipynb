{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('dataProcessed.csv').dropna()\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXBODYSIZE = 300\n",
    "MAXHEADSIZE = 100\n",
    "EMBEDDINGDIM = 300\n",
    "\n",
    "Stances = {'agree', 'disgree', 'discuss', 'unrelated'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lol bye memory :,( \n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('GensimVectors/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Stance</th>\n",
       "      <th>headProcessed</th>\n",
       "      <th>bodyProcessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>soldier shoot parliament lock down after gunfi...</td>\n",
       "      <td>a small meteorit crash into a wood area in nic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>tourist dub spider man after spider burrow und...</td>\n",
       "      <td>a small meteorit crash into a wood area in nic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>luke somer in fail rescu attempt in yemen</td>\n",
       "      <td>a small meteorit crash into a wood area in nic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>break soldier shoot at war memori in ottawa</td>\n",
       "      <td>a small meteorit crash into a wood area in nic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>giant catfish weigh stone catch in itali be th...</td>\n",
       "      <td>a small meteorit crash into a wood area in nic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                        articleBody  \\\n",
       "0        0  A small meteorite crashed into a wooded area i...   \n",
       "1        0  A small meteorite crashed into a wooded area i...   \n",
       "2        0  A small meteorite crashed into a wooded area i...   \n",
       "3        0  A small meteorite crashed into a wooded area i...   \n",
       "4        0  A small meteorite crashed into a wooded area i...   \n",
       "\n",
       "                                            Headline     Stance  \\\n",
       "0  Soldier shot, Parliament locked down after gun...  unrelated   \n",
       "1  Tourist dubbed ‘Spider Man’ after spider burro...  unrelated   \n",
       "2  Luke Somers 'killed in failed rescue attempt i...  unrelated   \n",
       "3   BREAKING: Soldier shot at War Memorial in Ottawa  unrelated   \n",
       "4  Giant 8ft 9in catfish weighing 19 stone caught...  unrelated   \n",
       "\n",
       "                                       headProcessed  \\\n",
       "0  soldier shoot parliament lock down after gunfi...   \n",
       "1  tourist dub spider man after spider burrow und...   \n",
       "2          luke somer in fail rescu attempt in yemen   \n",
       "3        break soldier shoot at war memori in ottawa   \n",
       "4  giant catfish weigh stone catch in itali be th...   \n",
       "\n",
       "                                       bodyProcessed  \n",
       "0  a small meteorit crash into a wood area in nic...  \n",
       "1  a small meteorit crash into a wood area in nic...  \n",
       "2  a small meteorit crash into a wood area in nic...  \n",
       "3  a small meteorit crash into a wood area in nic...  \n",
       "4  a small meteorit crash into a wood area in nic...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Body ID                                      bodyProcessed\n",
      "0        0  a small meteorit crash into a wood area in nic...\n",
      "1        4  last week we hint at what wa to come as ebola ...\n",
      "2        5  newser wonder how long a quarter pounder with ...\n",
      "3        6  post photo of a child onlin isi support announ...\n",
      "4        7  at least suspect boko haram insurg be kill in ...\n",
      "   Body ID                                      headProcessed     Stance\n",
      "0        0  soldier shoot parliament lock down after gunfi...  unrelated\n",
      "1        0  tourist dub spider man after spider burrow und...  unrelated\n",
      "2        0          luke somer in fail rescu attempt in yemen  unrelated\n",
      "3        0        break soldier shoot at war memori in ottawa  unrelated\n",
      "4        0  giant catfish weigh stone catch in itali be th...  unrelated\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# unique bodies\n",
    "Bodydf = df.groupby('Body ID').first()[['bodyProcessed']]\n",
    "Bodydf.reset_index(inplace=True)\n",
    "print(Bodydf.head(5))\n",
    "\n",
    "# unique Heads\n",
    "Headdf = df[['Body ID','headProcessed','Stance']]\n",
    "print(Headdf.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>headProcessed</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>soldier shoot parliament lock down after gunfi...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>tourist dub spider man after spider burrow und...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>luke somer in fail rescu attempt in yemen</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>break soldier shoot at war memori in ottawa</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>giant catfish weigh stone catch in itali be th...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                      headProcessed     Stance\n",
       "0        0  soldier shoot parliament lock down after gunfi...  unrelated\n",
       "1        0  tourist dub spider man after spider burrow und...  unrelated\n",
       "2        0          luke somer in fail rescu attempt in yemen  unrelated\n",
       "3        0        break soldier shoot at war memori in ottawa  unrelated\n",
       "4        0  giant catfish weigh stone catch in itali be th...  unrelated"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Headdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' set maximum dimension '''\n",
    "colIndex = Bodydf.columns.get_loc(\"bodyProcessed\")\n",
    "for index,row in Bodydf.iterrows():\n",
    "    Bodydf.iat[index, colIndex] = \" \".join([word for word in row['bodyProcessed'].split(' ') if word in model][:MAXBODYSIZE])\n",
    "Bodydf.to_csv('TrainBodyEmbed.csv',index=False)\n",
    "    \n",
    "colIndex = Headdf.columns.get_loc(\"headProcessed\")\n",
    "for index,row in Headdf.iterrows():\n",
    "    Headdf.iat[index, colIndex] = \" \".join([word for word in row['headProcessed'].split(' ') if word in model][:MAXHEADSIZE])\n",
    "Headdf.to_csv('TrainHeadEmbed.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalText = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalWordsForEmbedding(dfHead, dfBody):\n",
    "    global totalText\n",
    "    for index, row in dfHead.iterrows():\n",
    "        totalText.append(row['headProcessed'].split(' '))\n",
    "    for index, row in dfBody.iterrows():\n",
    "        totalText.append(row['bodyProcessed'].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalWordsForEmbedding(Headdf, Bodydf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Body ID                                      bodyProcessed\n",
      "0        1  ha deni isra report state that he offer to ext...\n",
      "1        2  a bereav afghan mother take reveng on the tali...\n",
      "2        3  cnbc be report tesla ha choose nevada as the s...\n",
      "3       12  a version of the iphon be say to be in develop...\n",
      "4       19  gr editor s note there be no report in the wes...\n",
      "   Body ID                                      headProcessed\n",
      "0        1        appl instal safe to protect gold watch edit\n",
      "1        1  deni claim he will give sinai land to palestinian\n",
      "2        1       appl to keep gold watch edit in special safe\n",
      "3        1  appl store to keep gold edit appl watch in cus...\n",
      "4        1  south korean woman hair by robot vacuum cleane...\n"
     ]
    }
   ],
   "source": [
    "''' CONDENSED FOR TEST '''\n",
    "\n",
    "df = pd.read_csv('testProcessed.csv').dropna()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "Bodydf = df.groupby('Body ID').first()[['bodyProcessed']]\n",
    "Bodydf.reset_index(inplace=True)\n",
    "print(Bodydf.head(5))\n",
    "\n",
    "# unique Heads\n",
    "Headdf = df[['Body ID','headProcessed']]\n",
    "print(Headdf.head(5))\n",
    "''' set maximum dimension '''\n",
    "colIndex = Bodydf.columns.get_loc(\"bodyProcessed\")\n",
    "for index,row in Bodydf.iterrows():\n",
    "    Bodydf.iat[index, colIndex] = \" \".join([word for word in row['bodyProcessed'].split(' ') if word in model][:MAXBODYSIZE])\n",
    "Bodydf.to_csv('TestBodyEmbed.csv',index=False)\n",
    "    \n",
    "colIndex = Headdf.columns.get_loc(\"headProcessed\")\n",
    "for index,row in Headdf.iterrows():\n",
    "    Headdf.iat[index, colIndex] = \" \".join([word for word in row['headProcessed'].split(' ') if word in model][:MAXHEADSIZE])\n",
    "Headdf.to_csv('TestHeadEmbed.csv',index=False)\n",
    "\n",
    "totalWordsForEmbedding(Headdf, Bodydf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8384 8384 77970\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(totalText)\n",
    "wordIndexs = tokenizer.word_index\n",
    "vocabSize = tokenizer.word_counts\n",
    "print(len(wordIndexs), len(vocabSize), tokenizer.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isi</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "the    1\n",
       "in     2\n",
       "be     3\n",
       "on     4\n",
       "for    5\n",
       "isi    6\n",
       "that   7\n",
       "say    8\n",
       "it     9\n",
       "have  10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordIndexsdf = pd.DataFrame.from_dict(wordIndexs, orient='index')\n",
    "wordIndexsdf.to_csv('wordIndexs.csv',index=False)\n",
    "wordIndexsdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.104980</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.053467</td>\n",
       "      <td>-0.067383</td>\n",
       "      <td>-0.120605</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>-0.118652</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>-0.030151</td>\n",
       "      <td>-0.013000</td>\n",
       "      <td>0.016357</td>\n",
       "      <td>-0.018311</td>\n",
       "      <td>0.014832</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>0.047607</td>\n",
       "      <td>-0.068848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>-0.108887</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>0.020752</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>-0.088867</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>0.061279</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.047607</td>\n",
       "      <td>0.014465</td>\n",
       "      <td>-0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.228516</td>\n",
       "      <td>-0.088379</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.150391</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.058350</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109863</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.045410</td>\n",
       "      <td>0.214844</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>-0.139648</td>\n",
       "      <td>-0.212891</td>\n",
       "      <td>0.188477</td>\n",
       "      <td>-0.145508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.026733</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.027832</td>\n",
       "      <td>0.204102</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.161133</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026855</td>\n",
       "      <td>-0.027954</td>\n",
       "      <td>0.030884</td>\n",
       "      <td>0.040527</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>-0.029419</td>\n",
       "      <td>-0.070801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.011780</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>-0.063965</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022583</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>-0.082520</td>\n",
       "      <td>0.081543</td>\n",
       "      <td>0.007935</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>0.024170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.124512</td>\n",
       "      <td>-0.042480</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>0.184570</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.226562</td>\n",
       "      <td>-0.126953</td>\n",
       "      <td>0.049316</td>\n",
       "      <td>-0.056396</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035400</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.079590</td>\n",
       "      <td>-0.134766</td>\n",
       "      <td>-0.200195</td>\n",
       "      <td>-0.009583</td>\n",
       "      <td>-0.162109</td>\n",
       "      <td>-0.388672</td>\n",
       "      <td>-0.064941</td>\n",
       "      <td>0.314453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.015747</td>\n",
       "      <td>-0.028320</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>0.050293</td>\n",
       "      <td>-0.110352</td>\n",
       "      <td>0.031738</td>\n",
       "      <td>-0.014221</td>\n",
       "      <td>-0.089844</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011292</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>-0.020630</td>\n",
       "      <td>-0.019409</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>0.006866</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>-0.148438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.036133</td>\n",
       "      <td>-0.121094</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>0.114258</td>\n",
       "      <td>-0.172852</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>0.173828</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015503</td>\n",
       "      <td>0.050049</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>-0.015869</td>\n",
       "      <td>-0.046143</td>\n",
       "      <td>-0.265625</td>\n",
       "      <td>-0.011780</td>\n",
       "      <td>-0.086426</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>0.027344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.084473</td>\n",
       "      <td>-0.000353</td>\n",
       "      <td>0.053223</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>-0.148438</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>0.056396</td>\n",
       "      <td>-0.064941</td>\n",
       "      <td>0.065918</td>\n",
       "      <td>0.018799</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032959</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>-0.157227</td>\n",
       "      <td>0.009705</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.118652</td>\n",
       "      <td>0.017090</td>\n",
       "      <td>0.060791</td>\n",
       "      <td>-0.108887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.139648</td>\n",
       "      <td>-0.034668</td>\n",
       "      <td>-0.053711</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>-0.036865</td>\n",
       "      <td>-0.025757</td>\n",
       "      <td>0.004852</td>\n",
       "      <td>-0.083496</td>\n",
       "      <td>0.008179</td>\n",
       "      <td>0.324219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049072</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.080566</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>0.077637</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>-0.071777</td>\n",
       "      <td>-0.118164</td>\n",
       "      <td>-0.002457</td>\n",
       "      <td>-0.072266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "1   0.080078  0.104980  0.049805  0.053467 -0.067383 -0.120605  0.035156   \n",
       "2   0.070312  0.086914  0.087891  0.062500  0.069336 -0.108887 -0.081543   \n",
       "3  -0.228516 -0.088379  0.127930  0.150391 -0.073242  0.086426  0.063965   \n",
       "4   0.026733 -0.090820  0.027832  0.204102  0.006226 -0.090332  0.022583   \n",
       "5  -0.011780 -0.047363  0.044678  0.063477 -0.018188 -0.063965 -0.001312   \n",
       "6  -0.124512 -0.042480  0.120117  0.184570 -0.132812  0.226562 -0.126953   \n",
       "7  -0.015747 -0.028320  0.083496  0.050293 -0.110352  0.031738 -0.014221   \n",
       "8  -0.036133 -0.121094  0.133789  0.114258 -0.172852  0.082031  0.218750   \n",
       "9   0.084473 -0.000353  0.053223  0.099609 -0.148438 -0.062500  0.056396   \n",
       "10 -0.139648 -0.034668 -0.053711  0.179688 -0.036865 -0.025757  0.004852   \n",
       "\n",
       "         7         8         9    ...       290       291       292       293  \\\n",
       "1  -0.118652  0.043945  0.030151  ... -0.071289 -0.030151 -0.013000  0.016357   \n",
       "2  -0.154297  0.020752  0.131836  ... -0.168945 -0.088867 -0.080566  0.064941   \n",
       "3   0.096680  0.058350  0.143555  ... -0.109863  0.064941  0.117188  0.045410   \n",
       "4  -0.161133  0.132812  0.061035  ...  0.026855 -0.027954  0.030884  0.040527   \n",
       "5  -0.072266  0.064453  0.086426  ... -0.022583  0.003723 -0.082520  0.081543   \n",
       "6   0.049316 -0.056396  0.100098  ... -0.035400  0.091797  0.079590 -0.134766   \n",
       "7  -0.089844  0.117676  0.118164  ... -0.011292 -0.015625 -0.033447 -0.020630   \n",
       "8  -0.136719  0.173828  0.182617  ...  0.015503  0.050049  0.069336 -0.015869   \n",
       "9  -0.064941  0.065918  0.018799  ... -0.032959  0.147461 -0.157227  0.009705   \n",
       "10 -0.083496  0.008179  0.324219  ... -0.049072  0.063477  0.080566  0.088867   \n",
       "\n",
       "         294       295       296       297       298       299  \n",
       "1  -0.018311  0.014832  0.005005  0.003662  0.047607 -0.068848  \n",
       "2   0.061279 -0.047363 -0.058838 -0.047607  0.014465 -0.062500  \n",
       "3   0.214844  0.042969 -0.139648 -0.212891  0.188477 -0.145508  \n",
       "4  -0.130859  0.083008  0.015747 -0.116699 -0.029419 -0.070801  \n",
       "5   0.007935  0.000477  0.018433  0.071289 -0.034912  0.024170  \n",
       "6  -0.200195 -0.009583 -0.162109 -0.388672 -0.064941  0.314453  \n",
       "7  -0.019409  0.063965  0.020142  0.006866  0.061035 -0.148438  \n",
       "8  -0.046143 -0.265625 -0.011780 -0.086426  0.143555  0.027344  \n",
       "9  -0.136719  0.031982  0.118652  0.017090  0.060791 -0.108887  \n",
       "10  0.077637 -0.046631 -0.071777 -0.118164 -0.002457 -0.072266  \n",
       "\n",
       "[10 rows x 300 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddingVector = {}\n",
    "for word, index in wordIndexs.items():\n",
    "    if word != '':\n",
    "        embeddingVector[index] = model[word]\n",
    "embeddingdf = pd.DataFrame.from_dict(embeddingVector, orient='index')\n",
    "embeddingdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingdf.to_csv('embeddingVectors.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBodyDf = pd.read_csv('TrainBodyEmbed.csv')\n",
    "trainHeadDf = pd.read_csv('TrainHeadEmbed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>bodyProcessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>small crash into wood area in capit overnight ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>last week we hint at what wa come as ebola fea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>newser wonder how long quarter pounder with ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>post photo child onlin isi support announc tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>at least suspect haram be kill in clash betwee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>there be so much fake stuff on the internet in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>cnn crash down in late saturday night caus lou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>move over netflix hulu word ha it that amazon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>we ve all see the depict god as with flow mane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>soldier ha be shoot at canada s nation war wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                      bodyProcessed\n",
       "0        0  small crash into wood area in capit overnight ...\n",
       "1        4  last week we hint at what wa come as ebola fea...\n",
       "2        5  newser wonder how long quarter pounder with ca...\n",
       "3        6  post photo child onlin isi support announc tha...\n",
       "4        7  at least suspect haram be kill in clash betwee...\n",
       "5        8  there be so much fake stuff on the internet in...\n",
       "6        9  cnn crash down in late saturday night caus lou...\n",
       "7       10  move over netflix hulu word ha it that amazon ...\n",
       "8       11  we ve all see the depict god as with flow mane...\n",
       "9       13  soldier ha be shoot at canada s nation war wit..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainBodyDf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>headProcessed</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>soldier shoot parliament lock down after erupt...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>tourist dub spider man after spider burrow und...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>luke in fail attempt in yemen</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>break soldier shoot at war in ottawa</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>giant catfish weigh stone catch in be think be...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                      headProcessed     Stance\n",
       "0        0  soldier shoot parliament lock down after erupt...  unrelated\n",
       "1        0  tourist dub spider man after spider burrow und...  unrelated\n",
       "2        0                      luke in fail attempt in yemen  unrelated\n",
       "3        0               break soldier shoot at war in ottawa  unrelated\n",
       "4        0  giant catfish weigh stone catch in be think be...  unrelated"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainHeadDf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>bodyProcessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>small crash into wood area in capit overnight ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>last week we hint at what wa come as ebola fea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>newser wonder how long quarter pounder with ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>post photo child onlin isi support announc tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>at least suspect haram be kill in clash betwee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                      bodyProcessed\n",
       "0        0  small crash into wood area in capit overnight ...\n",
       "1        4  last week we hint at what wa come as ebola fea...\n",
       "2        5  newser wonder how long quarter pounder with ca...\n",
       "3        6  post photo child onlin isi support announc tha...\n",
       "4        7  at least suspect haram be kill in clash betwee..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainBodyDf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Takes a minute or two '''\n",
    "def CreateTrainingData(bodydf, headdf):\n",
    "    heads = []\n",
    "    bodies = []\n",
    "    stances = []\n",
    "    stancesLookup = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    for index, row in headdf.iterrows():\n",
    "        if not pd.isna(row['headProcessed']):\n",
    "            heads.append(row['headProcessed'].split(\" \"))\n",
    "            bodies.append(bodydf.loc[bodydf['Body ID'] == int(row['Body ID'])].iloc[0]['bodyProcessed'][0].split(\" \"))\n",
    "            stances.append(stancesLookup[row['Stance'].strip()])\n",
    "    heads = tokenizer.texts_to_sequences(heads)\n",
    "    bodies = tokenizer.texts_to_sequences(bodies)\n",
    "    heads = pad_sequences(heads,maxlen = MAXHEADSIZE,padding = 'post')\n",
    "    bodies = pad_sequences(bodies,maxlen = MAXBODYSIZE,padding = 'post')\n",
    "    stances = to_categorical(stances, num_classes=4)\n",
    "    return heads,bodies,stances\n",
    "heads,bodies,stances = CreateTrainingData(trainBodyDf, trainHeadDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49952"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Input, Lambda\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import concatenate,dot\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import pydotplus\n",
    "import pydot\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import keras\n",
    "keras.utils.vis_utils.pydot = pydot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedMatrix = np.zeros((len(tokenizer.word_counts)+1 , EMBEDDINGDIM))\n",
    "for word, index in wordIndexs.items():\n",
    "    if word != '':\n",
    "        embedMatrix[index] = embeddingVector[tokenizer.texts_to_sequences([word])[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leo/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/leo/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "InputHead (InputLayer)          (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "InputBody (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             2515500     InputHead[0][0]                  \n",
      "                                                                 InputBody[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128)          186880      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128)          186880      embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 257)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          25800       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            404         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,915,464\n",
      "Trainable params: 399,964\n",
      "Non-trainable params: 2,515,500\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "InputHead = Input(shape=(MAXBODYSIZE,), dtype='int32', name='InputHead')\n",
    "InputBody = Input(shape=(MAXHEADSIZE,), dtype='int32', name='InputBody')\n",
    "Embeddings = Embedding(len(wordIndexs) + 1, EMBEDDINGDIM, weights=[embedMatrix],trainable=False)\n",
    "EmbedHead = Embeddings(InputHead)\n",
    "EmbedBody = Embeddings(InputBody)\n",
    "\n",
    "\n",
    "\n",
    "LSTMHead = Bidirectional(LSTM(64,dropout=0.2, recurrent_dropout=0.2, name='LSTMHead'))(EmbedHead)\n",
    "LSTMBody = Bidirectional(LSTM(64,dropout=0.2, recurrent_dropout=0.2, name='LSTMBody'))(EmbedBody)\n",
    "\n",
    "DotLayer = dot([LSTMHead,LSTMBody],axes = 1, normalize=True)\n",
    "\n",
    "Concat = concatenate([LSTMHead,LSTMBody,DotLayer])\n",
    "\n",
    "DenseLayer = Dense(100,activation='relu')(Concat)\n",
    "DenseLayer = Dropout(0.3)(DenseLayer)\n",
    "DenseLayer = Dense(4,activation='softmax')(DenseLayer)\n",
    "LSTMNetwork = Model(inputs=[InputHead,InputBody], outputs=[DenseLayer])\n",
    "LSTMNetwork.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "print(LSTMNetwork.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" CANT GET TO WORK? shows network graph, currently a keras bug \\nplot_model(LSTMNetwork, to_file='LSTMNetworkMap.png')\\nSVG(model_to_dot(LSTMNetwork).create(prog='dot', format='svg'))\\n\\nIf you are getting errors here for pydot install graphviz & pydot & pydotplus\\nGraphViz needs to be installed using apt-get / brew\\notherwise comment out sainvg\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' CANT GET TO WORK? shows network graph, currently a keras bug \n",
    "plot_model(LSTMNetwork, to_file='LSTMNetworkMap.png')\n",
    "SVG(model_to_dot(LSTMNetwork).create(prog='dot', format='svg'))\n",
    "\n",
    "If you are getting errors here for pydot install graphviz & pydot & pydotplus\n",
    "GraphViz needs to be installed using apt-get / brew\n",
    "otherwise comment out sainvg\\\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33301"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert(len(heads) == len(stances))\n",
    "splitNum = int(len(heads) * 2 / 3)\n",
    "splitNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leo/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 33301 samples, validate on 16651 samples\n",
      "Epoch 1/4\n",
      "33301/33301 [==============================] - 653s 20ms/step - loss: 0.7185 - acc: 0.7740 - val_loss: 0.9485 - val_acc: 0.6358\n",
      "Epoch 2/4\n",
      "33301/33301 [==============================] - 648s 19ms/step - loss: 0.6630 - acc: 0.7792 - val_loss: 0.9634 - val_acc: 0.6358\n",
      "Epoch 3/4\n",
      "33301/33301 [==============================] - 648s 19ms/step - loss: 0.6396 - acc: 0.7791 - val_loss: 0.9868 - val_acc: 0.6353\n",
      "Epoch 4/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.6107 - acc: 0.7810 - val_loss: 0.9703 - val_acc: 0.6360\n",
      "Train on 33301 samples, validate on 16651 samples\n",
      "Epoch 1/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.5780 - acc: 0.7856 - val_loss: 1.0282 - val_acc: 0.6388\n",
      "Epoch 2/4\n",
      "33301/33301 [==============================] - 648s 19ms/step - loss: 0.5487 - acc: 0.7938 - val_loss: 0.9537 - val_acc: 0.6389\n",
      "Epoch 3/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.5208 - acc: 0.7983 - val_loss: 1.0214 - val_acc: 0.6416\n",
      "Epoch 4/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.4994 - acc: 0.8052 - val_loss: 1.0135 - val_acc: 0.6388\n",
      "Train on 33301 samples, validate on 16651 samples\n",
      "Epoch 1/4\n",
      "33301/33301 [==============================] - 648s 19ms/step - loss: 0.4806 - acc: 0.8089 - val_loss: 1.0519 - val_acc: 0.6360\n",
      "Epoch 2/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.4669 - acc: 0.8133 - val_loss: 1.0764 - val_acc: 0.6389\n",
      "Epoch 3/4\n",
      "33301/33301 [==============================] - 648s 19ms/step - loss: 0.4504 - acc: 0.8185 - val_loss: 1.1116 - val_acc: 0.6379\n",
      "Epoch 4/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.4415 - acc: 0.8194 - val_loss: 1.0891 - val_acc: 0.6389\n",
      "Train on 33301 samples, validate on 16651 samples\n",
      "Epoch 1/4\n",
      "33301/33301 [==============================] - 646s 19ms/step - loss: 0.4313 - acc: 0.8238 - val_loss: 1.1152 - val_acc: 0.6358\n",
      "Epoch 2/4\n",
      "33301/33301 [==============================] - 646s 19ms/step - loss: 0.4210 - acc: 0.8257 - val_loss: 1.1853 - val_acc: 0.6425\n",
      "Epoch 3/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.4119 - acc: 0.8260 - val_loss: 1.2000 - val_acc: 0.6323\n",
      "Epoch 4/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.4078 - acc: 0.8296 - val_loss: 1.2028 - val_acc: 0.6350\n",
      "Train on 33301 samples, validate on 16651 samples\n",
      "Epoch 1/4\n",
      "33301/33301 [==============================] - 646s 19ms/step - loss: 0.4006 - acc: 0.8308 - val_loss: 1.2773 - val_acc: 0.6366\n",
      "Epoch 2/4\n",
      "33301/33301 [==============================] - 646s 19ms/step - loss: 0.3921 - acc: 0.8303 - val_loss: 1.2999 - val_acc: 0.6361\n",
      "Epoch 3/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.3868 - acc: 0.8336 - val_loss: 1.2780 - val_acc: 0.6388\n",
      "Epoch 4/4\n",
      "33301/33301 [==============================] - 648s 19ms/step - loss: 0.3866 - acc: 0.8350 - val_loss: 1.2428 - val_acc: 0.6386\n",
      "Train on 33301 samples, validate on 16651 samples\n",
      "Epoch 1/4\n",
      "33301/33301 [==============================] - 646s 19ms/step - loss: 0.3804 - acc: 0.8342 - val_loss: 1.3147 - val_acc: 0.6385\n",
      "Epoch 2/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.3750 - acc: 0.8356 - val_loss: 1.3413 - val_acc: 0.6355\n",
      "Epoch 3/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.3710 - acc: 0.8379 - val_loss: 1.2976 - val_acc: 0.6339\n",
      "Epoch 4/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.3668 - acc: 0.8379 - val_loss: 1.4097 - val_acc: 0.6353\n",
      "Train on 33301 samples, validate on 16651 samples\n",
      "Epoch 1/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.3648 - acc: 0.8381 - val_loss: 1.4603 - val_acc: 0.6355\n",
      "Epoch 2/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.3606 - acc: 0.8379 - val_loss: 1.5526 - val_acc: 0.6378\n",
      "Epoch 3/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.3573 - acc: 0.8404 - val_loss: 1.5664 - val_acc: 0.6352\n",
      "Epoch 4/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.3535 - acc: 0.8403 - val_loss: 1.5628 - val_acc: 0.6347\n",
      "Train on 33301 samples, validate on 16651 samples\n",
      "Epoch 1/4\n",
      "33301/33301 [==============================] - 646s 19ms/step - loss: 0.3502 - acc: 0.8411 - val_loss: 1.6643 - val_acc: 0.6357\n",
      "Epoch 2/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.3495 - acc: 0.8426 - val_loss: 1.5485 - val_acc: 0.6323\n",
      "Epoch 3/4\n",
      "33301/33301 [==============================] - 648s 19ms/step - loss: 0.3477 - acc: 0.8420 - val_loss: 1.5889 - val_acc: 0.6347\n",
      "Epoch 4/4\n",
      "33301/33301 [==============================] - 646s 19ms/step - loss: 0.3443 - acc: 0.8438 - val_loss: 1.5541 - val_acc: 0.6310\n",
      "Train on 33301 samples, validate on 16651 samples\n",
      "Epoch 1/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.3443 - acc: 0.8446 - val_loss: 1.6421 - val_acc: 0.6350\n",
      "Epoch 2/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.3399 - acc: 0.8443 - val_loss: 1.6617 - val_acc: 0.6350\n",
      "Epoch 3/4\n",
      "33301/33301 [==============================] - 648s 19ms/step - loss: 0.3393 - acc: 0.8447 - val_loss: 1.6858 - val_acc: 0.6362\n",
      "Epoch 4/4\n",
      "33301/33301 [==============================] - 649s 19ms/step - loss: 0.3342 - acc: 0.8456 - val_loss: 1.7255 - val_acc: 0.6336\n",
      "Train on 33301 samples, validate on 16651 samples\n",
      "Epoch 1/4\n",
      "33301/33301 [==============================] - 646s 19ms/step - loss: 0.3371 - acc: 0.8439 - val_loss: 1.6318 - val_acc: 0.6340\n",
      "Epoch 2/4\n",
      "33301/33301 [==============================] - 648s 19ms/step - loss: 0.3344 - acc: 0.8472 - val_loss: 1.7958 - val_acc: 0.6362\n",
      "Epoch 3/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.3325 - acc: 0.8452 - val_loss: 1.6612 - val_acc: 0.6348\n",
      "Epoch 4/4\n",
      "33301/33301 [==============================] - 647s 19ms/step - loss: 0.3287 - acc: 0.8466 - val_loss: 1.7553 - val_acc: 0.6317\n"
     ]
    }
   ],
   "source": [
    "TrainHeadSplit, TrainBodySplit, TrainStanceSplit = bodies[:splitNum], heads[:splitNum], stances[:splitNum]\n",
    "TestHeadSplit, TestBodySplit, TestStanceSplit = bodies[splitNum:], heads[splitNum:], stances[splitNum:]\n",
    "\n",
    "\n",
    "# validation_data = ([TestHeadSplit, TestBodySplit],[TestStanceSplit])\n",
    "for i in range(10):\n",
    "    LSTMNetwork.fit([TrainHeadSplit, TrainBodySplit],[TrainStanceSplit], validation_data = ([TestHeadSplit, TestBodySplit],[TestStanceSplit]), epochs=4, batch_size=128,verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMNetwork.save('model.5h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
