{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Globals '''\n",
    "MAXBODYSIZE = 500\n",
    "MAXHEADSIZE = 50\n",
    "EMBEDDINGDIM = 300\n",
    "Stances = {'agree', 'disgree', 'discuss', 'unrelated'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load data sets '''\n",
    "trainBodiesDF = pd.read_csv('./DefaultFiles/train_bodies.csv')\n",
    "trainHeadDF = pd.read_csv('./DefaultFiles/train_stances.csv')\n",
    "testBodiesDF = pd.read_csv('./DefaultFiles/test_bodies.csv')\n",
    "testHeadDF = pd.read_csv('./DefaultFiles/test_stances_unlabeled.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Body ID                                        articleBody\n",
      "0        0  A small meteorite crashed into a wooded area i...\n",
      "1        4  Last week we hinted at what was to come as Ebo...\n",
      "2        5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
      "   index  Body ID                                           Headline  \\\n",
      "0      0        0  Soldier shot, Parliament locked down after gun...   \n",
      "1      1        0  Tourist dubbed ‘Spider Man’ after spider burro...   \n",
      "2      2        0  Luke Somers 'killed in failed rescue attempt i...   \n",
      "\n",
      "      Stance  \n",
      "0  unrelated  \n",
      "1  unrelated  \n",
      "2  unrelated  \n",
      "   Body ID                                        articleBody\n",
      "0        1  Al-Sisi has denied Israeli reports stating tha...\n",
      "1        2  A bereaved Afghan mother took revenge on the T...\n",
      "2        3  CNBC is reporting Tesla has chosen Nevada as t...\n",
      "                                            Headline  Body ID\n",
      "0  Ferguson riots: Pregnant woman loses eye after...     2008\n",
      "1  Crazy Conservatives Are Sure a Gitmo Detainee ...     1550\n",
      "2  A Russian Guy Says His Justin Bieber Ringtone ...        2\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    Cleaning \n",
    "    - drop heads with no reference body\n",
    "    - drop null heads\n",
    "    - reset indexes to accomodate change\n",
    "'''\n",
    "totalTrain = pd.merge(trainBodiesDF, trainHeadDF, on='Body ID')\n",
    "trainBodiesDF = totalTrain.groupby('Body ID').first()[['articleBody']]\n",
    "trainHeadDF = totalTrain[['Body ID','Headline','Stance']]\n",
    "trainHeadDF = trainHeadDF.dropna()\n",
    "trainBodiesDF.reset_index(inplace=True)\n",
    "trainHeadDF.reset_index(inplace=True)\n",
    "print(trainBodiesDF.head(3))\n",
    "print(trainHeadDF.head(3))\n",
    "print(testBodiesDF.head(3))\n",
    "print(testHeadDF.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Load Pretrained Word2Vec by Google\n",
    "    Word2Vec is a shallow neural network ot produce word embeddings\n",
    "    The primary goal is vectorize the linguistic context of the word\n",
    "    You can download from here:\n",
    "    https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "'''\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2Vec = KeyedVectors.load_word2vec_format('GensimVectors/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    For downloading for nltk\n",
    "    import \n",
    "    on first time download the following packages\n",
    "\n",
    "    nltk.download()\n",
    "    select d\n",
    "    download packages ['punkt', wordnet', 'stopwords']\n",
    "'''\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import re\n",
    "\n",
    "'''\n",
    "    Processing text\n",
    "    1. Split into words i.e [[word,word],[word,word,word]]\n",
    "    2. Stem - chop of ends\n",
    "    3. Lemmatise - remove inflection endings and return to base citionary\n",
    "    4. remove stopwards\n",
    "    5. only take words containing only letters and contained in Word2Vec vocab\n",
    "'''\n",
    "def process( text):\n",
    "    out = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    outout = []\n",
    "    for word in word_tokenize(text):\n",
    "        word = word.strip().lower()\n",
    "        word = stemmer.stem(word)\n",
    "        word = lemmatizer.lemmatize(word, wordnet.VERB)\n",
    "        # major speed gain only testing for letters\n",
    "        word = word.replace(\"n't\", 'not')\n",
    "        word = word.replace(\"'m\", 'am')\n",
    "        word = word.replace(\"'ve'\", 'have')\n",
    "        word = word.replace(\"'d\", 'would')\n",
    "        word = word.replace(\"'ll\", \"will\")\n",
    "        if word != '' and word.isalpha() and word in word2Vec:\n",
    "            out.append(word.lower())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Loop through all four data frames and process the text\n",
    "    ~ Will take approximately 2 minutes\n",
    "'''\n",
    "for index, row in trainBodiesDF.iterrows():\n",
    "    trainBodiesDF.iat[index, trainBodiesDF.columns.get_loc(\"articleBody\")] = \" \".join(process(row['articleBody']))\n",
    "for index, row in trainHeadDF.iterrows():\n",
    "    trainHeadDF.iat[index, trainHeadDF.columns.get_loc(\"Headline\")] = \" \".join(process(row['Headline']))\n",
    "for index, row in testBodiesDF.iterrows():\n",
    "    testBodiesDF.iat[index, testBodiesDF.columns.get_loc(\"articleBody\")] = \" \".join(process(row['articleBody']))\n",
    "for index, row in testHeadDF.iterrows():\n",
    "    testHeadDF.iat[index, testHeadDF.columns.get_loc(\"Headline\")] = \" \".join(process(row['Headline']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Body ID                                        articleBody\n",
      "0        0  small crash into wood area in capit overnight ...\n",
      "1        4  last week we hint at what wa come as ebola fea...\n",
      "2        5  newser wonder how long quarter pounder with ca...\n",
      "   index  Body ID                                           Headline  \\\n",
      "0      0        0  soldier shoot parliament lock down after erupt...   \n",
      "1      1        0  tourist dub spider man after spider burrow und...   \n",
      "2      2        0                      luke in fail attempt in yemen   \n",
      "\n",
      "      Stance  \n",
      "0  unrelated  \n",
      "1  unrelated  \n",
      "2  unrelated  \n",
      "   Body ID                                        articleBody\n",
      "0        1  ha report state that he offer extend the gaza ...\n",
      "1        2  afghan mother take on the taliban after watch ...\n",
      "2        3  cnbc be report tesla ha choose nevada as the s...\n",
      "                                            Headline  Body ID\n",
      "0  ferguson riot pregnant woman lose eye after co...     2008\n",
      "1                           be sure gitmo kill foley     1550\n",
      "2  russian guy say hi justin bieber rington save ...        2\n"
     ]
    }
   ],
   "source": [
    "print(trainBodiesDF.head(3))\n",
    "print(trainHeadDF.head(3))\n",
    "print(testBodiesDF.head(3))\n",
    "print(testHeadDF.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1683, 2) (49972, 4) (904, 2) (25413, 2)\n"
     ]
    }
   ],
   "source": [
    "''' Save a checkpoint '''\n",
    "trainBodiesDF.to_csv('ProcessedTrainBodies.csv',index=False)\n",
    "trainHeadDF.to_csv('ProcessedTrainHead.csv',index=False)\n",
    "testBodiesDF.to_csv('ProcessedTestBodies.csv',index=False)\n",
    "testHeadDF.to_csv('ProcessedTestHead.csv',index=False)\n",
    "print(trainBodiesDF.shape, trainHeadDF.shape, testBodiesDF.shape, testHeadDF.shape, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:  9309\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Create and train tokenizer\n",
    "    Tokenizer is utilised to create numerical representations of the data\n",
    "'''\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "totalText = []\n",
    "for index, row in trainBodiesDF.iterrows():\n",
    "    totalText.append(row['articleBody'])\n",
    "for index, row in trainHeadDF.iterrows():\n",
    "    totalText.append(row['Headline'])\n",
    "for index, row in testBodiesDF.iterrows():\n",
    "    totalText.append(row['articleBody'])\n",
    "for index, row in testHeadDF.iterrows():\n",
    "    totalText.append(row['Headline'])\n",
    "    \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(totalText)\n",
    "wordIndexs = tokenizer.word_index\n",
    "vocabSize = tokenizer.word_counts\n",
    "print('Vocab Size: ',len(wordIndexs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "the  1\n",
       "be   2\n",
       "in   3\n",
       "on   4\n",
       "for  5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    utilise tokenizer and save word representations\n",
    "'''\n",
    "wordIndexsdf = pd.DataFrame.from_dict(wordIndexs, orient='index')\n",
    "wordIndexsdf.to_csv('wordIndexs.csv',index=False)\n",
    "wordIndexsdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.104980</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.053467</td>\n",
       "      <td>-0.067383</td>\n",
       "      <td>-0.120605</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>-0.118652</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>-0.030151</td>\n",
       "      <td>-0.013000</td>\n",
       "      <td>0.016357</td>\n",
       "      <td>-0.018311</td>\n",
       "      <td>0.014832</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>0.047607</td>\n",
       "      <td>-0.068848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.228516</td>\n",
       "      <td>-0.088379</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.150391</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.058350</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109863</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.045410</td>\n",
       "      <td>0.214844</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>-0.139648</td>\n",
       "      <td>-0.212891</td>\n",
       "      <td>0.188477</td>\n",
       "      <td>-0.145508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>-0.108887</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>0.020752</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>-0.088867</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>0.061279</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.047607</td>\n",
       "      <td>0.014465</td>\n",
       "      <td>-0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.026733</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.027832</td>\n",
       "      <td>0.204102</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.161133</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026855</td>\n",
       "      <td>-0.027954</td>\n",
       "      <td>0.030884</td>\n",
       "      <td>0.040527</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>-0.029419</td>\n",
       "      <td>-0.070801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.011780</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>-0.063965</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022583</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>-0.082520</td>\n",
       "      <td>0.081543</td>\n",
       "      <td>0.007935</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>0.024170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "1  0.080078  0.104980  0.049805  0.053467 -0.067383 -0.120605  0.035156   \n",
       "2 -0.228516 -0.088379  0.127930  0.150391 -0.073242  0.086426  0.063965   \n",
       "3  0.070312  0.086914  0.087891  0.062500  0.069336 -0.108887 -0.081543   \n",
       "4  0.026733 -0.090820  0.027832  0.204102  0.006226 -0.090332  0.022583   \n",
       "5 -0.011780 -0.047363  0.044678  0.063477 -0.018188 -0.063965 -0.001312   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "1 -0.118652  0.043945  0.030151  ... -0.071289 -0.030151 -0.013000  0.016357   \n",
       "2  0.096680  0.058350  0.143555  ... -0.109863  0.064941  0.117188  0.045410   \n",
       "3 -0.154297  0.020752  0.131836  ... -0.168945 -0.088867 -0.080566  0.064941   \n",
       "4 -0.161133  0.132812  0.061035  ...  0.026855 -0.027954  0.030884  0.040527   \n",
       "5 -0.072266  0.064453  0.086426  ... -0.022583  0.003723 -0.082520  0.081543   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "1 -0.018311  0.014832  0.005005  0.003662  0.047607 -0.068848  \n",
       "2  0.214844  0.042969 -0.139648 -0.212891  0.188477 -0.145508  \n",
       "3  0.061279 -0.047363 -0.058838 -0.047607  0.014465 -0.062500  \n",
       "4 -0.130859  0.083008  0.015747 -0.116699 -0.029419 -0.070801  \n",
       "5  0.007935  0.000477  0.018433  0.071289 -0.034912  0.024170  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddingVector = {}\n",
    "for word, index in wordIndexs.items():\n",
    "    if word != '':\n",
    "        embeddingVector[index] = word2Vec[word]\n",
    "embeddingdf = pd.DataFrame.from_dict(embeddingVector, orient='index')\n",
    "embeddingdf.to_csv('embeddingVectors.csv',index=False)\n",
    "embeddingdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01574707, -0.02832031,  0.08349609, ...,  0.00686646,\n",
       "         0.06103516, -0.1484375 ],\n",
       "       [ 0.08447266, -0.00035286,  0.05322266, ...,  0.01708984,\n",
       "         0.06079102, -0.10888672],\n",
       "       [-0.03613281, -0.12109375,  0.13378906, ..., -0.08642578,\n",
       "         0.14355469,  0.02734375],\n",
       "       ...,\n",
       "       [ 0.20605469, -0.29882812,  0.06298828, ...,  0.13671875,\n",
       "        -0.17675781, -0.11523438],\n",
       "       [ 0.00595093,  0.00102997, -0.19921875, ..., -0.3046875 ,\n",
       "         0.05151367, -0.17382812],\n",
       "       [-0.01330566,  0.0088501 ,  0.01184082, ..., -0.07373047,\n",
       "        -0.08056641,  0.0703125 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddingMatrix = embeddingdf.to_numpy()\n",
    "embeddingMatrix[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Loaded Function\n",
    "    Purposes\n",
    "    - Change pandas dataframe to trainable / testable numpy data\n",
    "    - texts to sequences - convert words into their appropriate numerical representation\n",
    "    - pad_sequences - convert all vectors into desired length (increase / decrease size)\n",
    "    - for train data - convert stances into numerical representation\n",
    "'''\n",
    "def CreateNetworkData(bodydf, headdf, stance):\n",
    "    heads = []\n",
    "    bodies = []\n",
    "    stances = []\n",
    "    stancesLookup = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    for index, row in headdf.iterrows():\n",
    "        # don't drop rows in test\n",
    "        if not stance:\n",
    "            if pd.isna(row['Headline']):\n",
    "                heads.append([])\n",
    "            else:\n",
    "                heads.append(row['Headline'].split(\" \"))\n",
    "            try:\n",
    "                bodies.append(bodydf.loc[bodydf['Body ID'] == int(row['Body ID'])].iloc[0]['articleBody'][0].split(\" \"))\n",
    "            except Exception:\n",
    "                print(bodydf.loc[bodydf['Body ID'] == int(row['Body ID'])].iloc[0]['articleBody'])\n",
    "            if stance:\n",
    "                stances.append(stancesLookup[row['Stance'].strip()])\n",
    "        else:\n",
    "            if not pd.isna(row['Headline']):\n",
    "                heads.append(row['Headline'].split(\" \"))\n",
    "                try:\n",
    "                    bodies.append(bodydf.loc[bodydf['Body ID'] == int(row['Body ID'])].iloc[0]['articleBody'][0].split(\" \"))\n",
    "                except Exception:\n",
    "                    print(row['Body ID'])\n",
    "                    bodies.append([])\n",
    "                if stance:\n",
    "                    stances.append(stancesLookup[row['Stance'].strip()])\n",
    "    heads = tokenizer.texts_to_sequences(heads)\n",
    "    bodies = tokenizer.texts_to_sequences(bodies)\n",
    "    heads = pad_sequences(heads,maxlen = MAXHEADSIZE,padding = 'post')\n",
    "    bodies = pad_sequences(bodies,maxlen = MAXBODYSIZE,padding = 'post')\n",
    "    if stance:\n",
    "        stances = to_categorical(stances, num_classes=4)\n",
    "    return heads,bodies,stances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Create data structures for lstm nework\n",
    "'''\n",
    "trainHeads,trainBodies,trainStances = CreateNetworkData(trainBodiesDF, trainHeadDF, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Embedding, Input\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.recurrent import LSTM \n",
    "from keras.layers import concatenate \n",
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leo/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/leo/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "InputHead (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "InputBody (InputLayer)          (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             2792700     InputHead[0][0]                  \n",
      "                                                                 InputBody[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128)          186880      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128)          186880      embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          32896       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            516         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,199,872\n",
      "Trainable params: 407,172\n",
      "Non-trainable params: 2,792,700\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Bidirectional LSTM used\n",
    "    inputs are concatenated and feed into a two layer dense network with dropout\n",
    "    Please refer to report for further information about method\n",
    "'''\n",
    "\n",
    "InputHead = Input(shape=(MAXHEADSIZE,), dtype='int32', name='InputHead')\n",
    "InputBody = Input(shape=(MAXBODYSIZE,), dtype='int32', name='InputBody')\n",
    "Embeddings = Embedding(len(wordIndexs), EMBEDDINGDIM, weights=[embeddingMatrix],trainable=False)\n",
    "EmbedHead = Embeddings(InputHead)\n",
    "EmbedBody = Embeddings(InputBody)\n",
    "\n",
    "LSTMHead = Bidirectional(LSTM(64,dropout=0.2, recurrent_dropout=0.2, name='LSTMHead'))(EmbedHead)\n",
    "LSTMBody = Bidirectional(LSTM(64,dropout=0.2, recurrent_dropout=0.2, name='LSTMBody'))(EmbedBody)\n",
    "\n",
    "Concat = concatenate([LSTMHead,LSTMBody])\n",
    "\n",
    "DenseLayer = Dense(128,activation='relu')(Concat)\n",
    "DenseLayer = Dropout(0.4)(DenseLayer)\n",
    "DenseLayer = Dense(4,activation='softmax')(DenseLayer)\n",
    "LSTMNetwork = Model(inputs=[InputHead,InputBody], outputs=[DenseLayer])\n",
    "LSTMNetwork.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "print(LSTMNetwork.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leo/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 1132s 23ms/step - loss: 0.7947 - acc: 0.7295\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 1125s 23ms/step - loss: 0.7481 - acc: 0.7312\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 1126s 23ms/step - loss: 0.7273 - acc: 0.7314\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 1128s 23ms/step - loss: 0.7054 - acc: 0.7324\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 1127s 23ms/step - loss: 0.6860 - acc: 0.7360\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 1129s 23ms/step - loss: 0.6680 - acc: 0.7388\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 1128s 23ms/step - loss: 0.6541 - acc: 0.7422\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 1128s 23ms/step - loss: 0.6419 - acc: 0.7437\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 1129s 23ms/step - loss: 0.6305 - acc: 0.7460\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 1126s 23ms/step - loss: 0.6241 - acc: 0.7482\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 1129s 23ms/step - loss: 0.6144 - acc: 0.7496\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 1128s 23ms/step - loss: 0.6052 - acc: 0.7514\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 1129s 23ms/step - loss: 0.5975 - acc: 0.7534\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 1129s 23ms/step - loss: 0.5911 - acc: 0.7546\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 1129s 23ms/step - loss: 0.5860 - acc: 0.7566\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 1130s 23ms/step - loss: 0.5787 - acc: 0.7579\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 1130s 23ms/step - loss: 0.5745 - acc: 0.7584\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 1129s 23ms/step - loss: 0.5684 - acc: 0.7607\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 1131s 23ms/step - loss: 0.5600 - acc: 0.7633\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 1129s 23ms/step - loss: 0.5575 - acc: 0.7649\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 1134s 23ms/step - loss: 0.5503 - acc: 0.7653\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 1128s 23ms/step - loss: 0.5467 - acc: 0.7675\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 1129s 23ms/step - loss: 0.5442 - acc: 0.7661\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 1129s 23ms/step - loss: 0.5397 - acc: 0.7671\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 1129s 23ms/step - loss: 0.5373 - acc: 0.7692\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 1129s 23ms/step - loss: 0.5298 - acc: 0.7705\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 1130s 23ms/step - loss: 0.5284 - acc: 0.7714\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 1131s 23ms/step - loss: 0.5276 - acc: 0.7727\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 1130s 23ms/step - loss: 0.5226 - acc: 0.7716\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 1134s 23ms/step - loss: 0.5203 - acc: 0.7740\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 1141s 23ms/step - loss: 0.5151 - acc: 0.7766\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 1130s 23ms/step - loss: 0.5140 - acc: 0.7765\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 1133s 23ms/step - loss: 0.5085 - acc: 0.7755\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 1131s 23ms/step - loss: 0.5086 - acc: 0.7759\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 1131s 23ms/step - loss: 0.5060 - acc: 0.7779\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 1131s 23ms/step - loss: 0.5040 - acc: 0.7788\n",
      "Epoch 1/4\n",
      "49972/49972 [==============================] - 1131s 23ms/step - loss: 0.5005 - acc: 0.7800\n",
      "Epoch 2/4\n",
      "49972/49972 [==============================] - 1131s 23ms/step - loss: 0.4985 - acc: 0.7798\n",
      "Epoch 3/4\n",
      "49972/49972 [==============================] - 1132s 23ms/step - loss: 0.4978 - acc: 0.7788\n",
      "Epoch 4/4\n",
      "49972/49972 [==============================] - 1131s 23ms/step - loss: 0.4942 - acc: 0.7807\n"
     ]
    }
   ],
   "source": [
    "''' Train the model ~ takes roughly 10 hours '''\n",
    "for i in range(10):\n",
    "    LSTMNetwork.fit([trainHeads, trainBodies],[trainStances], epochs=4, batch_size=128,verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Save all your hard work '''\n",
    "LSTMNetwork.save('finalModel.5h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "LSTMNetwork = load_model('finalModel.5h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create test data appropriate for model '''\n",
    "testHeads,testBodies,out = CreateNetworkData(testBodiesDF, testHeadDF, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Predict the test data '''\n",
    "predictions = LSTMNetwork.predict([testHeads, testBodies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Ferguson riots: Pregnant woman loses eye after cops fire BEAN BAG round through car window',\n",
       "        2008, 'unrelated'],\n",
       "       ['Crazy Conservatives Are Sure a Gitmo Detainee Killed James Foley',\n",
       "        1550, 'unrelated'],\n",
       "       ['A Russian Guy Says His Justin Bieber Ringtone Saved Him From A Bear Attack',\n",
       "        2, 'unrelated'],\n",
       "       ...,\n",
       "       ['The success of the Affordable Care Act is a hugely inconvenient truth for its opponents',\n",
       "        2584, 'unrelated'],\n",
       "       ['The success of the Affordable Care Act is a hugely inconvenient truth for its opponents',\n",
       "        2585, 'unrelated'],\n",
       "       ['The success of the Affordable Care Act is a hugely inconvenient truth for its opponents',\n",
       "        2586, 'unrelated']], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "    Convert predictions into csv approrpiate for evaluating\n",
    "    - take argmax of predictions to determine classifcaiton\n",
    "    - map these back to the appropriate stance in word\n",
    "'''\n",
    "testStancesDf = pd.read_csv('./DefaultFiles/test_stances_unlabeled.csv')\n",
    "reverseMap = np.vectorize(lambda label: { 0:'unrelated', 1:'agree', 2:'disagree', 3:'discuss'}[label])\n",
    "testPredsFinal = np.column_stack((testStancesDf, reverseMap(np.argmax(predictions,axis=1))))\n",
    "testPredsFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' One liner to save dataframe appropriately '''\n",
    "pd.DataFrame(testPredsFinal, columns=['Headline', 'Body ID', 'Stance']).to_csv('testPredictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    29     |     2     |    35     |   1837    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |     2     |     0     |     5     |    690    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    23     |     1     |    175    |   4265    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    107    |     1     |    379    |   17862   |\n",
      "-------------------------------------------------------------\n",
      "ACCURACY: 0.711\n",
      "\n",
      "MAX  - the best possible score (100% accuracy)\n",
      "NULL - score as if all predicted stances were unrelated\n",
      "TEST - score based on the provided predictions\n",
      "\n",
      "||    MAX    ||    NULL   ||    TEST   ||\n",
      "|| 11651.25  ||  4587.25  ||  4686.5   ||\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Print out the confusion matrix of the predictions and evaluate score\n",
    "'''\n",
    "\n",
    "%run -i scorer.py DefaultFiles/competition_test_stances.csv testPredictions.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
