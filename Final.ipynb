{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Globals '''\n",
    "MAXBODYSIZE = 500\n",
    "MAXHEADSIZE = 50\n",
    "EMBEDDINGDIM = 300\n",
    "Stances = {'agree', 'disgree', 'discuss', 'unrelated'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load data sets '''\n",
    "trainBodiesDF = pd.read_csv('./DefaultFiles/train_bodies.csv')\n",
    "trainHeadDF = pd.read_csv('./DefaultFiles/train_stances.csv')\n",
    "testBodiesDF = pd.read_csv('./DefaultFiles/test_bodies.csv')\n",
    "testHeadDF = pd.read_csv('./DefaultFiles/test_stances_unlabeled.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Cleaning \n",
    "    - drop heads with no reference body\n",
    "    - drop null heads\n",
    "    - reset indexes to accomodate change\n",
    "'''\n",
    "totalTrain = pd.merge(trainBodiesDF, trainHeadDF, on='Body ID')\n",
    "trainBodiesDF = totalTrain.groupby('Body ID').first()[['articleBody']]\n",
    "trainHeadDF = totalTrain[['Body ID','Headline','Stance']]\n",
    "trainHeadDF = trainHeadDF.dropna()\n",
    "trainBodiesDF.reset_index(inplace=True)\n",
    "trainHeadDF.reset_index(inplace=True)\n",
    "print(trainBodiesDF.head(3))\n",
    "print(trainHeadDF.head(3))\n",
    "print(testBodiesDF.head(3))\n",
    "print(testHeadDF.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Load Pretrained Word2Vec by Google\n",
    "    Word2Vec is a shallow neural network ot produce word embeddings\n",
    "    The primary goal is vectorize the linguistic context of the word\n",
    "    You can download from here:\n",
    "    https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "'''\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2Vec = KeyedVectors.load_word2vec_format('GensimVectors/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    For downloading for nltk\n",
    "    import \n",
    "    on first time download the following packages\n",
    "\n",
    "    nltk.download()\n",
    "    select d\n",
    "    download packages ['punkt', wordnet', 'stopwords']\n",
    "'''\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import re\n",
    "\n",
    "'''\n",
    "    Processing text\n",
    "    1. Split into words i.e [[word,word],[word,word,word]]\n",
    "    2. Stem - chop of ends\n",
    "    3. Lemmatise - remove inflection endings and return to base citionary\n",
    "    4. remove stopwards\n",
    "    5. only take words containing only letters and contained in Word2Vec vocab\n",
    "'''\n",
    "def process( text):\n",
    "    out = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    outout = []\n",
    "    for word in word_tokenize(text):\n",
    "        word = word.strip().lower()\n",
    "        word = stemmer.stem(word)\n",
    "        word = lemmatizer.lemmatize(word, wordnet.VERB)\n",
    "        # major speed gain only testing for letters\n",
    "        word = word.replace(\"n't\", 'not')\n",
    "        word = word.replace(\"'m\", 'am')\n",
    "        word = word.replace(\"'ve'\", 'have')\n",
    "        word = word.replace(\"'d\", 'would')\n",
    "        word = word.replace(\"'ll\", \"will\")\n",
    "        if word != '' and word.isalpha() and word in word2Vec:\n",
    "            out.append(word.lower())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Loop through all four data frames and process the text\n",
    "    ~ Will take approximately 2 minutes\n",
    "'''\n",
    "for index, row in trainBodiesDF.iterrows():\n",
    "    trainBodiesDF.iat[index, trainBodiesDF.columns.get_loc(\"articleBody\")] = \" \".join(process(row['articleBody']))\n",
    "for index, row in trainHeadDF.iterrows():\n",
    "    trainHeadDF.iat[index, trainHeadDF.columns.get_loc(\"Headline\")] = \" \".join(process(row['Headline']))\n",
    "for index, row in testBodiesDF.iterrows():\n",
    "    testBodiesDF.iat[index, testBodiesDF.columns.get_loc(\"articleBody\")] = \" \".join(process(row['articleBody']))\n",
    "for index, row in testHeadDF.iterrows():\n",
    "    testHeadDF.iat[index, testHeadDF.columns.get_loc(\"Headline\")] = \" \".join(process(row['Headline']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainBodiesDF.head(3))\n",
    "print(trainHeadDF.head(3))\n",
    "print(testBodiesDF.head(3))\n",
    "print(testHeadDF.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Save a checkpoint '''\n",
    "trainBodiesDF.to_csv('ProcessedTrainBodies.csv',index=False)\n",
    "trainHeadDF.to_csv('ProcessedTrainHead.csv',index=False)\n",
    "testBodiesDF.to_csv('ProcessedTestBodies.csv',index=False)\n",
    "testHeadDF.to_csv('ProcessedTestHead.csv',index=False)\n",
    "print(trainBodiesDF.shape, trainHeadDF.shape, testBodiesDF.shape, testHeadDF.shape, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Create and train tokenizer\n",
    "    Tokenizer is utilised to create numerical representations of the data\n",
    "'''\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "totalText = []\n",
    "for index, row in trainBodiesDF.iterrows():\n",
    "    totalText.append(row['articleBody'])\n",
    "for index, row in trainHeadDF.iterrows():\n",
    "    totalText.append(row['Headline'])\n",
    "for index, row in testBodiesDF.iterrows():\n",
    "    totalText.append(row['articleBody'])\n",
    "for index, row in testHeadDF.iterrows():\n",
    "    totalText.append(row['Headline'])\n",
    "    \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(totalText)\n",
    "wordIndexs = tokenizer.word_index\n",
    "vocabSize = tokenizer.word_counts\n",
    "print('Vocab Size: ',len(wordIndexs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    utilise tokenizer and save word representations\n",
    "'''\n",
    "wordIndexsdf = pd.DataFrame.from_dict(wordIndexs, orient='index')\n",
    "wordIndexsdf.to_csv('wordIndexs.csv',index=False)\n",
    "wordIndexsdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingVector = {}\n",
    "for word, index in wordIndexs.items():\n",
    "    if word != '':\n",
    "        embeddingVector[index] = word2Vec[word]\n",
    "embeddingdf = pd.DataFrame.from_dict(embeddingVector, orient='index')\n",
    "embeddingdf.to_csv('embeddingVectors.csv',index=False)\n",
    "embeddingdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingMatrix = embeddingdf.to_numpy()\n",
    "embeddingMatrix[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Loaded Function\n",
    "    Purposes\n",
    "    - Change pandas dataframe to trainable / testable numpy data\n",
    "    - texts to sequences - convert words into their appropriate numerical representation\n",
    "    - pad_sequences - convert all vectors into desired length (increase / decrease size)\n",
    "    - for train data - convert stances into numerical representation\n",
    "'''\n",
    "def CreateNetworkData(bodydf, headdf, stance):\n",
    "    heads = []\n",
    "    bodies = []\n",
    "    stances = []\n",
    "    stancesLookup = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    for index, row in headdf.iterrows():\n",
    "        # don't drop rows in test\n",
    "        if not stance:\n",
    "            if pd.isna(row['Headline']):\n",
    "                heads.append([])\n",
    "            else:\n",
    "                heads.append(row['Headline'].split(\" \"))\n",
    "            try:\n",
    "                bodies.append(bodydf.loc[bodydf['Body ID'] == int(row['Body ID'])].iloc[0]['articleBody'][0].split(\" \"))\n",
    "            except Exception:\n",
    "                print(bodydf.loc[bodydf['Body ID'] == int(row['Body ID'])].iloc[0]['articleBody'])\n",
    "            if stance:\n",
    "                stances.append(stancesLookup[row['Stance'].strip()])\n",
    "        else:\n",
    "            if not pd.isna(row['Headline']):\n",
    "                heads.append(row['Headline'].split(\" \"))\n",
    "                bodies.append(bodydf.loc[bodydf['Body ID'] == int(row['Body ID'])].iloc[0]['articleBody'][0].split(\" \"))\n",
    "                if stance:\n",
    "                    stances.append(stancesLookup[row['Stance'].strip()])\n",
    "    heads = tokenizer.texts_to_sequences(heads)\n",
    "    bodies = tokenizer.texts_to_sequences(bodies)\n",
    "    heads = pad_sequences(heads,maxlen = MAXHEADSIZE,padding = 'post')\n",
    "    bodies = pad_sequences(bodies,maxlen = MAXBODYSIZE,padding = 'post')\n",
    "    if stance:\n",
    "        stances = to_categorical(stances, num_classes=4)\n",
    "    return heads,bodies,stances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Create data structures for lstm nework\n",
    "'''\n",
    "trainHeads,trainBodies,trainStances = CreateNetworkData(trainBodiesDF, trainHeadDF, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Embedding, Input\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.recurrent import LSTM \n",
    "from keras.layers import concatenate \n",
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Bidirectional LSTM used\n",
    "    inputs are concatenated and feed into a two layer dense network with dropout\n",
    "    Please refer to report for further information about method\n",
    "'''\n",
    "\n",
    "InputHead = Input(shape=(MAXHEADSIZE,), dtype='int32', name='InputHead')\n",
    "InputBody = Input(shape=(MAXBODYSIZE,), dtype='int32', name='InputBody')\n",
    "Embeddings = Embedding(len(wordIndexs), EMBEDDINGDIM, weights=[embeddingMatrix],trainable=False)\n",
    "EmbedHead = Embeddings(InputHead)\n",
    "EmbedBody = Embeddings(InputBody)\n",
    "\n",
    "LSTMHead = Bidirectional(LSTM(64,dropout=0.2, recurrent_dropout=0.2, name='LSTMHead'))(EmbedHead)\n",
    "LSTMBody = Bidirectional(LSTM(64,dropout=0.2, recurrent_dropout=0.2, name='LSTMBody'))(EmbedBody)\n",
    "\n",
    "Concat = concatenate([LSTMHead,LSTMBody])\n",
    "\n",
    "DenseLayer = Dense(128,activation='relu')(Concat)\n",
    "DenseLayer = Dropout(0.4)(DenseLayer)\n",
    "DenseLayer = Dense(4,activation='softmax')(DenseLayer)\n",
    "LSTMNetwork = Model(inputs=[InputHead,InputBody], outputs=[DenseLayer])\n",
    "LSTMNetwork.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "print(LSTMNetwork.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Train the model ~ takes roughly 10 hours '''\n",
    "for i in range(10):\n",
    "    LSTMNetwork.fit([trainHeads, trainBodies],[trainStances], epochs=4, batch_size=128,verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Save all your hard work '''\n",
    "LSTMNetwork.save('finalModel.5h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "LSTMNetwork = load_model('finalModel.5h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create test data appropriate for model '''\n",
    "testHeads,testBodies,out = CreateNetworkData(testBodiesDF, testHeadDF, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Predict the test data '''\n",
    "predictions = LSTMNetwork.predict([testHeads, testBodies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Convert predictions into csv approrpiate for evaluating\n",
    "    - take argmax of predictions to determine classifcaiton\n",
    "    - map these back to the appropriate stance in word\n",
    "'''\n",
    "testStancesDf = pd.read_csv('./DefaultFiles/test_stances_unlabeled.csv')\n",
    "reverseMap = np.vectorize(lambda label: { 0:'unrelated', 1:'agree', 2:'disagree', 3:'discuss'}[label])\n",
    "testPredsFinal = np.column_stack((testStancesDf, reverseMap(np.argmax(predictions,axis=1))))\n",
    "testPredsFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' One liner to save dataframe appropriately '''\n",
    "pd.DataFrame(testPredsFinal, columns=['Headline', 'Body ID', 'Stance']).to_csv('testPredictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Print out the confusion matrix of the predictions and evaluate score\n",
    "'''\n",
    "\n",
    "%run -i scorer.py DefaultFiles/competition_test_stances.csv testPredictions.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
